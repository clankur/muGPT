# python -m train --config-name=c4_a100x8x4_37m +paths.model_name=270m
defaults:
  - wikitext103_base
  - _self_

num_hosts: 4

mesh:
  d: 16
  t: 1

training:
  warmup_steps: 240
  steps:        2400
  steps_for_lr: 2400
  learning_rate: 7.3e-3
  tokens:
    batch: 256
  use_grad_clip: true
  use_gpu: false
  use_checkpoint: false

model:
  d_model: 256
  n_q_per_kv: 1
  n_kv: 16
  d_head: 64
  d_ff: 1024
  layers: 8
  vocab: 32768

  base:
    d_model: 256
    n_q_per_kv: 1
    n_kv: 16
    d_head: 64
    d_ff: 1024

  rope_max_timescale: 10000
  cope_n_pos_max: 64
  apply_cope: true
  apply_rope: true

  a_attn: 1.
  a_output: 1.
  zero_queries: true
  zero_unembed: true

  parameterization: "sp"
  fully_aligned: false
  gamma_embed: 1.
  gamma_hidden: 1.
  gamma_unembed: 1.

checkpoint_interval: 9200

paths:
  # root_working_dir can also be a path on a local filesystem
  root_working_dir: "/tmp"
