# Command to run on your CPU:
#   XLA_FLAGS=--xla_force_host_platform_device_count=8 python -m train --config-name=local_test_synthetic +paths.model_name=synthetic_000

defaults:
- base
- _self_

training:
  warmup_steps: 10
  steps:        100
  steps_for_lr: 100
  tokens:
    batch: 64
    len: 64
  use_grad_clip: true
  use_gpu: false

model:
  d_model: 256
  n_q_per_kv: 2
  n_kv: 2
  d_head: 32
  layers: 2
  vocab: 32768 
  d_ff: 1024
  rope_max_timescale: 256

  # parameters for mixattention
  window_size: 16 
  n_kv_reuses: 2
  shared_kv_idx:
    - 0
    - 1 
  sa_layers:
    - 0 

  a_attn: 1
  a_output: 1
  zero_queries: true
  zero_unembed: true
  base:
    d_model: 256
    n_q_per_kv: 2
    n_kv: 2
    d_head: 32
    d_ff: 1024
  parameterization: "sp" 
  fully_aligned: false
  gamma_embed: 1. 
  gamma_hidden: 1. 
  gamma_unembed: 1. 


paths:
  root_working_dir: '/tmp'

checkpoint_interval: 10
num_hosts: 1

mesh:
  d: 4
  t: 2

flat_tokens:
  filespec: 'synthetic_dataset.zarr'
  streams: 1
  read_blocks_per_shuffle_buffer: 128
  sequences_per_read_block: 1024
  seed: 0
  sequence_packing: true
